{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "py38",
      "display_name": "Python 3.8"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How to run other kernels inside Google Colab\n",
        "\n",
        "**Author:** Rafael Ito  \n",
        "**E-mail:** ito.rafael@gmail.com  \n",
        "**Description:** This notebook shows how to run another kernel in Google Colab. In this case, we will run Python 3.8 inside Colab, but the same strategy can be done to run condes in [JavaScript](https://colab.research.google.com/gist/korakot/22abd6eccac229e9cb9a027b088b50d6/notebook.ipynb), [Java](https://colab.research.google.com/github/vistec-AI/colab/blob/master/ijava.ipynb), [Go](https://colab.research.google.com/drive/1-6XkA5OhEA6lMW9DvH4_AcXndC7WppJx), etc.  \n",
        "**Source:** https://stackoverflow.com/questions/60775160/install-python-3-8-kernel-in-google-colaboratory/71511943#71511943"
      ],
      "metadata": {
        "id": "gFijsaFdFPfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The steps:\n",
        "\n",
        "1. Create an empty notebook and download it (.ipynb extension) into your computer:\n",
        "File --> Download --> Download .ipynb)\n",
        "\n",
        "2. Open the downloaded notebook and edit the keys \"name\" and \"display_name\" (lines 10 & 11, respectively) under the section \"kernelspec\" as following:\n",
        "\n",
        "* Before:\n",
        "```\n",
        "\"kernelspec\": {\n",
        "  \"name\": \"python3\",\n",
        "  \"display_name\": \"Python 3\"\n",
        "},\n",
        "```\n",
        "\n",
        "* After:\n",
        "```\n",
        "\"kernelspec\": {\n",
        "  \"name\": \"py38\",\n",
        "  \"display_name\": \"Python 3.8\"\n",
        "},\n",
        "```\n",
        "\n",
        "3. Upload the notebook back to Google Colab:\n",
        "In Google Colab --> File --> Upload notebook --> Upload --> Choose File --> select the previous modified notebook\n",
        "\n",
        "4. Run all commands from this notebook to confirm that Python 3.8 is running now.\n",
        "\n",
        "5. Restart the runtime (Runtime --> Restart runtime).\n",
        "\n",
        "6. Reload the page (\"F5\" key).\n",
        "\n",
        "Troubleshooting:\n",
        "- Try restarting the runtime (Runtime --> Restart runtime)."
      ],
      "metadata": {
        "id": "FICz1xdjBjVT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxEBB8zQBKEx",
        "outputId": "0b804123-d005-4ef7-c7a7-d6e5df3fb0f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "version: 3.7.15 (default, Oct 12 2022, 19:14:55) \n",
            "[GCC 7.5.0]\n"
          ]
        }
      ],
      "source": [
        "# check version of Python\n",
        "import sys\n",
        "print(\"version:\", sys.version)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# list available kernels\n",
        "!jupyter kernelspec list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9_JQscrCNdB",
        "outputId": "9c33de3d-e61e-4951-df8a-47d8508e03f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available kernels:\n",
            "  ir         /usr/local/share/jupyter/kernels/ir\n",
            "  python3    /usr/local/share/jupyter/kernels/python3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install kernel for Python 3.8\n",
        "# if you want to see the output of the commands, delete the \"-q\"/\"--quiet\" flag and the \"&> /dev/null\" output redirections\n",
        "!wget -q -O mini.sh https://repo.anaconda.com/miniconda/Miniconda3-py38_4.8.2-Linux-x86_64.sh\n",
        "!chmod +x mini.sh\n",
        "!bash ./mini.sh -b -f -p /usr/local &> /dev/null\n",
        "!conda install --quiet jupyter -y &> /dev/null\n",
        "!conda install --quiet google-colab -c conda-forge -y &> /dev/null\n",
        "!python -m ipykernel install --name \"py38\" --user"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvwoeTL4BMJZ",
        "outputId": "49338496-a8ba-47bb-aea4-d5873ad9540e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installed kernelspec py38 in /root/.local/share/jupyter/kernels/py38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# list the available kernels again\n",
        "!jupyter kernelspec list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fjWWojwCK08",
        "outputId": "0fd2015e-49d7-4c6f-9f21-60cd0b83ab4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available kernels:\n",
            "  py38       /root/.local/share/jupyter/kernels/py38\n",
            "  ir         /usr/local/share/jupyter/kernels/ir\n",
            "  python3    /usr/local/share/jupyter/kernels/python3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before running the next cell, be sure to reload the page (\"F5\" key)."
      ],
      "metadata": {
        "id": "gtIGRlc-QEp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check version of Python\n",
        "import sys\n",
        "print(\"version:\", sys.version)"
      ],
      "metadata": {
        "id": "sTMrKFa3Bd83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28267532-511b-4d06-eccc-3cf635cca2f7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "version: 3.8.13 (default, Oct 21 2022, 23:50:54) \n",
            "[GCC 11.2.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install iNNvestigate\n",
        "!pip -q install innvestigate"
      ],
      "metadata": {
        "id": "ljaipYdMB8QQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34bb494b-982d-4d07-cfc7-4f5484c1af07"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 66 kB 2.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 578.1 MB 5.6 kB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 42.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 9.4 MB 40.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 36.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 14.1 MB 94 kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 59.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 45.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 438 kB 62.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 57 kB 4.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 47.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 50.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.5 MB 37.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 65 kB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 81 kB 9.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 124 kB 57.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 29.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 295 kB 75.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 54.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 965 kB 62.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 232 kB 63.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 51.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 781 kB 60.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 151 kB 46.2 MB/s \n",
            "\u001b[?25h  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Python version\n",
        "import sys\n",
        "print('Python version:', sys.version)"
      ],
      "metadata": {
        "id": "Y5H6s4jET-nI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41fb74de-31c2-4359-b7fa-6784f7c6e813"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version: 3.8.13 (default, Oct 21 2022, 23:50:54) \n",
            "[GCC 11.2.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# innvestigate version\n",
        "import innvestigate\n",
        "print('iNNvestigate version:', innvestigate.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7jjN3YIQ4eO",
        "outputId": "ea13df54-2af6-463b-bcca-1cd223ff09c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-11-10 21:38:54.970791: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-11-10 21:38:55.305151: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2022-11-10 21:38:57.054687: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2022-11-10 21:38:57.055061: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2022-11-10 21:38:57.055089: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iNNvestigate version: 2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TensorFlow version\n",
        "import tensorflow as tf\n",
        "print('TensorFlow version:', tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHG4Ws5IYC8F",
        "outputId": "bd855832-4fbb-4970-d7e3-878428034f3e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the iNNvestigate module\n",
        "# source: https://github.com/albermax/innvestigate/blob/master/examples/embedding_minimal_example.py\n",
        "# note: we disabled eager-execution for TF2/TF1 compatibility\n",
        "\n",
        "import numpy as np\n",
        "from keras import Sequential\n",
        "from keras.layers import Conv1D, Dense, Embedding, GlobalMaxPooling1D\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "import innvestigate\n",
        "\n",
        "# Create Keras Sequential Model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=219, output_dim=8))\n",
        "model.add(Conv1D(filters=64, kernel_size=8, padding=\"valid\", activation=\"relu\"))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(16, activation=\"relu\"))\n",
        "model.add(Dense(2, activation=None))\n",
        "\n",
        "# Analyze model\n",
        "model.predict(np.random.randint(1, 219, (1, 100)))  # [[0.04913538 0.04234646]]\n",
        "\n",
        "analyzer = innvestigate.create_analyzer(\n",
        "    \"lrp.epsilon\", model, neuron_selection_mode=\"max_activation\", **{\"epsilon\": 1}\n",
        ")\n",
        "a = analyzer.analyze(np.random.randint(1, 219, (1, 100)))\n",
        "print(a[0], a[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lubcKJM6YZrK",
        "outputId": "65035bcc-4e36-4797-aab7-38e6138e7704"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/site-packages/keras/engine/training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "2022-11-10 21:39:00.476492: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2022-11-10 21:39:00.476582: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (c7233dce6bf1): /proc/driver/nvidia/version does not exist\n",
            "2022-11-10 21:39:00.478249: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-11-10 21:39:00.487690: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  2.57487187e-09\n",
            "  3.74738462e-08  2.49277026e-08  4.72588724e-08  1.88555056e-08\n",
            "  2.42483704e-08  1.61984879e-08  5.13729645e-08 -1.80444708e-08\n",
            "  3.04448822e-09  4.95408958e-09  3.35270158e-08 -3.16386526e-08\n",
            "  1.88495495e-08  1.95436929e-08 -2.13395346e-08 -3.97483433e-08\n",
            " -2.86670065e-09  4.12342942e-08 -2.22243948e-08 -1.75046289e-08\n",
            "  1.82194100e-08  5.62264937e-08  3.97547737e-08  1.33167521e-08\n",
            " -9.01201673e-08  3.45909115e-08 -8.92238816e-08 -1.33557476e-08\n",
            " -3.69904818e-08  3.91957711e-09  1.94966816e-08 -3.81307750e-08\n",
            "  3.30807168e-08 -1.30973987e-08 -4.33523120e-08 -7.49123998e-08\n",
            " -1.06923110e-08  1.59465952e-08 -7.13346537e-08  4.34915677e-08\n",
            "  4.81025459e-08 -5.49884334e-08  2.77038161e-08  4.54529214e-09\n",
            " -4.95780483e-08  1.50371108e-07  8.41302921e-08  1.75977831e-07\n",
            "  6.36267501e-08  7.93531427e-08  1.81876601e-07  9.46121190e-08\n",
            "  4.34691572e-08  3.83101124e-08  1.23802295e-07 -5.19008623e-08\n",
            " -1.44495536e-08 -1.42823836e-07  8.69451284e-08  3.73392872e-08\n",
            " -1.82758662e-08 -5.14958209e-08 -1.85040783e-08 -1.07283228e-07\n",
            "  3.50701157e-08 -6.40740083e-08  1.23527153e-08  8.22057711e-09\n",
            "  1.57437485e-09 -1.70567560e-08  1.43955159e-09 -8.52848725e-09\n",
            " -1.05134612e-09 -1.88982199e-08 -1.94120275e-08  4.86891061e-09\n",
            " -3.23840332e-09 -8.32484659e-09 -5.16215550e-08  1.32872513e-08\n",
            " -2.42068161e-08  4.06500860e-08  6.62253399e-08  1.19354651e-08\n",
            " -2.30397426e-08 -2.32229631e-08  5.62722713e-09 -5.89478510e-08\n",
            " -2.17529177e-08  1.54082542e-08  3.94144228e-09 -2.81504491e-08\n",
            " -4.09519814e-08 -3.62185233e-08 -3.76845541e-08 -2.35325217e-08] (100,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End of notebook"
      ],
      "metadata": {
        "id": "qXks9PIcqvBF"
      }
    }
  ]
}